import requests, time, os, random, re

DIRECTOR_PROMPT = """
ä½ æ˜¯ä¸€ä½ Bç«™/YouTube é¡¶çº§å°é¢è®¾è®¡å¸ˆã€‚ä»»åŠ¡æ˜¯ã€æç‚¼è§†è§‰çˆ†ç‚¹ã€‘ã€‚
1. å¯»æ‰¾é’©å­ï¼šå‰§æœ¬é‡Œæœ€åç›´è§‰ã€æœ€ç¡¬æ ¸çš„æ¦‚å¿µã€‚
2. è§†è§‰éšå–»ï¼šå°†æŠ½è±¡æ¦‚å¿µå…·è±¡åŒ–ï¼ˆå¦‚ï¼šä»£ç æµä»é”®ç›˜ç‚¸è£‚ï¼‰ã€‚
3. ç”µå½±çº§è´¨æ„Ÿï¼šCinematic lighting, 8k resolution, photorealistic.
ã€ç¦ä»¤ã€‘âŒ ä¸¥ç¦ç”Ÿæˆä»»ä½•æ–‡å­—ã€‚âŒ ä¸¥ç¦å‡ºç°æ¨¡ç³Šä¸»ä½“ã€‚
ã€è¾“å‡ºã€‘ç›´æ¥è¾“å‡ºè‹±æ–‡ Promptï¼Œé€—å·åˆ†éš”ã€‚
"""

class AIEngine:
    def __init__(self, model="qwen2.5:7b"):
        self.ollama_url = "http://localhost:11434/api/generate"
        self.comfy_url = "http://127.0.0.1:8188"
        self.model = model

    def _clean(self, text):
        return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()

    def generate_text(self, prompt, system_prompt=None):
        full = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
        payload = {
            "model": self.model,
            "prompt": full,
            "stream": False,
            "options": {
                "num_ctx": 4096, "temperature": 0.8, "repeat_penalty": 1.3, 
                "stop": ["<|endoftext|>", "<|im_end|>", "Thank you"]
            }
        }
        try:
            res = requests.post(self.ollama_url, json=payload, timeout=600)
            if res.status_code == 200:
                return self._clean(res.json().get('response', ""))
            return f"Error: {res.status_code}"
        except Exception as e: return str(e)

    def generate_visual_prompt(self, script):
        prompt = f"ä»»åŠ¡ï¼šä¸ºä»¥ä¸‹å‰§æœ¬è®¾è®¡ä¸€å¼ Bç«™å°é¢åº•å›¾ï¼ˆåªè¦è‹±æ–‡Promptï¼Œä¸è¦æ–‡å­—ï¼‰ã€‚\nå‰§æœ¬ï¼š{script[:1500]}"
        return self.generate_text(prompt, system_prompt=DIRECTOR_PROMPT)

    def optimize_prompt(self, text):
        return self.generate_text(f"Translate to high-quality Flux prompt: {text}", system_prompt=DIRECTOR_PROMPT)

    def get_all_models(self):
        try:
            r = requests.get(f"{self.comfy_url}/object_info/CheckpointLoaderSimple", timeout=2)
            if r.status_code == 200:
                return r.json()['CheckpointLoaderSimple']['input']['required']['ckpt_name'][0]
        except: pass
        return []

    def _find_file(self, folder, extension, keywords):
        """è¾…åŠ©å‡½æ•°ï¼šåœ¨æŒ‡å®šæ–‡ä»¶å¤¹æ‰¾ç¬¦åˆå…³é”®è¯çš„æ–‡ä»¶"""
        # ComfyUI çš„æ ¹ç›®å½•é€šå¸¸åœ¨ä¸Šä¸€çº§æˆ–åŒçº§ï¼Œè¿™é‡Œå‡è®¾åœ¨é¡¹ç›®åŒçº§çš„ ComfyUI æ–‡ä»¶å¤¹
        # ä½ å¯ä»¥æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´æ­¤å¤„çš„ search_path
        search_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "ComfyUI", "models", folder)
        if not os.path.exists(search_path):
            return None
        for f in os.listdir(search_path):
            if f.endswith(extension) and any(k.lower() in f.lower() for k in keywords):
                return f
        return None

    def generate_image(self, prompt, save_dir):
        # 1. è‡ªåŠ¨æœç´¢æ¨¡å‹é›¶ä»¶
        unet = self._find_file("unet", ".gguf", ["flux", "q4"])
        clip_t5 = self._find_file("clip", ".safetensors", ["t5xxl"])
        clip_l = self._find_file("clip", ".safetensors", ["clip_l"])
        vae = self._find_file("vae", ".safetensors", ["ae", "flux"])

        # æ£€æŸ¥é›¶ä»¶æ˜¯å¦é½å…¨ï¼Œä¸å…¨å°±æŠ¥é”™
        if not all([unet, clip_t5, clip_l, vae]):
            error_msg = f"âŒ ç¼ºå°‘æ¨¡å‹æ–‡ä»¶ï¼è¯·æ£€æŸ¥ï¼šUnet({unet}), T5({clip_t5}), CLIP_L({clip_l}), VAE({vae})"
            print(error_msg)
            return None

        seed = random.randint(1, 10**14)
        print(f"ğŸš€ è‡ªåŠ¨åŠ è½½æˆåŠŸï¼æ­£åœ¨ç”Ÿæˆå°é¢...")
        print(f"ğŸ“¦ Unet: {unet} | Seed: {seed}")

        # 2. åŠ¨æ€æ„å»ºè“å›¾
        workflow = {
            "10": {"inputs": {"unet_name": unet}, "class_type": "UnetLoaderGGUF"},
            "11": {
                "inputs": {"clip_name1": clip_t5, "clip_name2": clip_l, "type": "flux"},
                "class_type": "DualCLIPLoader"
            },
            "12": {"inputs": {"vae_name": vae}, "class_type": "VAELoader"},
            "3": {
                "inputs": {
                    "seed": seed, "steps": 25, "cfg": 1.0, # æ­¥æ•°å·²è®¾ä¸º 25 ä»¥ä¿è¯ç”»è´¨
                    "sampler_name": "euler", "scheduler": "simple", "denoise": 1, 
                    "model": ["10", 0], "positive": ["6", 0], "negative": ["7", 0], "latent_image": ["5", 0]
                },
                "class_type": "KSampler"
            },
            "5": {"inputs": {"width": 1280, "height": 720, "batch_size": 1}, "class_type": "EmptyLatentImage"},
            "6": {"inputs": {"text": prompt, "clip": ["11", 0]}, "class_type": "CLIPTextEncode"},
            "7": {"inputs": {"text": "blurry, low quality", "clip": ["11", 0]}, "class_type": "CLIPTextEncode"},
            "8": {"inputs": {"samples": ["3", 0], "vae": ["12", 0]}, "class_type": "VAEDecode"},
            "9": {"inputs": {"filename_prefix": "Bili_Cover", "images": ["8", 0]}, "class_type": "SaveImage"}
        }

        # 3. å‘é€è¯·æ±‚ç»™ ComfyUI
        try:
            r = requests.post(f"{self.comfy_url}/prompt", json={"prompt": workflow}, timeout=10)
            if r.status_code != 200: return None
            pid = r.json()['prompt_id']
            
            start = time.time()
            while time.time() - start < 600: # å¢åŠ åˆ° 10 åˆ†é’Ÿè¶…æ—¶ï¼Œé˜²æ­¢ 4060 æ¸²æŸ“å¤§å›¾æ…¢
                time.sleep(2)
                try:
                    h = requests.get(f"{self.comfy_url}/history/{pid}").json()
                    if pid in h:
                        fname = h[pid]['outputs']['9']['images'][0]['filename']
                        res = requests.get(f"{self.comfy_url}/view", params={"filename": fname, "type": "output"})
                        path = os.path.join(save_dir, fname)
                        with open(path, 'wb') as f: f.write(res.content)
                        return path
                except: continue
        except Exception as e:
            print(f"è¿æ¥å¤±è´¥: {e}")
        return None